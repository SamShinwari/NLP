{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00yw1gO4_R3Z",
        "outputId": "3abda74e-2f7f-4550-dff3-d1d17b589427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "# Make sure standard punkt is downloaded\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dPcuMgQEXEg",
        "outputId": "2e461683-dfef-460d-b123-7535862aabed"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5E6bwh3B_R3b"
      },
      "outputs": [],
      "source": [
        "corpus=\"\"\"Hello Welcome,to Krish Naik's NLP Tutorials.\n",
        "Please do watch the entire course! to become expert in NLP.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7TDDbVU_R3c",
        "outputId": "06c913b9-0eae-46e9-d335-515e89a16a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome,to Krish Naik's NLP Tutorials.\n",
            "Please do watch the entire course! to become expert in NLP.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paragraph Tokenization**"
      ],
      "metadata": {
        "id": "oXXJWk3zEplt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PunktSentenceTokenizer()\n",
        "sentences = tokenizer.tokenize(corpus)\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lkdWD9hEjdN",
        "outputId": "97759e0a-f743-4de7-b973-126accb7bfd0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Hello Welcome,to Krish Naik's NLP Tutorials.\n",
            "2. Please do watch the entire course!\n",
            "3. to become expert in NLP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fBE0npnl_R3i"
      },
      "outputs": [],
      "source": [
        "## Tokenization\n",
        "## Paragraph-->words\n",
        "## sentence--->words\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Tokenization**"
      ],
      "metadata": {
        "id": "X7Fqc-nZFJGP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iEKaIkV8_R3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3ff931-3abc-401b-b209-ce0d3b73cf88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization:\n",
            "Sentence 1 words: ['Hello', 'Welcome', ',', 'to', 'Krish', 'Naik', \"'s\", 'NLP', 'Tutorials', '.']\n",
            "Sentence 2 words: ['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
            "Sentence 3 words: ['to', 'become', 'expert', 'in', 'NLP', '.']\n"
          ]
        }
      ],
      "source": [
        "# 2. Word Tokenizer (use TreebankWordTokenizer to bypass punkt_tab)\n",
        "word_tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "print(\"Word Tokenization:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    words = word_tokenizer.tokenize(sentence)\n",
        "    print(f\"Sentence {i} words: {words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "T3hnDCpj_R3k"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WyLebNEn_R3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ee2738-50d1-49b9-803f-adf57ebcdcf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Krish',\n",
              " 'Naik',\n",
              " \"'\",\n",
              " 's',\n",
              " 'NLP',\n",
              " 'Tutorials',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "wordpunct_tokenize(corpus)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}