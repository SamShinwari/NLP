{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00yw1gO4_R3Z",
        "outputId": "3abda74e-2f7f-4550-dff3-d1d17b589427"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "# Make sure standard punkt is downloaded\n",
        "nltk.download('punkt')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dPcuMgQEXEg",
        "outputId": "2e461683-dfef-460d-b123-7535862aabed"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5E6bwh3B_R3b"
      },
      "outputs": [],
      "source": [
        "corpus=\"\"\"Hello Welcome,to Krish Naik's NLP Tutorials.\n",
        "Please do watch the entire course! to become expert in NLP.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7TDDbVU_R3c",
        "outputId": "06c913b9-0eae-46e9-d335-515e89a16a8a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello Welcome,to Krish Naik's NLP Tutorials.\n",
            "Please do watch the entire course! to become expert in NLP.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paragraph Tokenization**"
      ],
      "metadata": {
        "id": "oXXJWk3zEplt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = PunktSentenceTokenizer()\n",
        "sentences = tokenizer.tokenize(corpus)\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7lkdWD9hEjdN",
        "outputId": "97759e0a-f743-4de7-b973-126accb7bfd0"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1. Hello Welcome,to Krish Naik's NLP Tutorials.\n",
            "2. Please do watch the entire course!\n",
            "3. to become expert in NLP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fBE0npnl_R3i"
      },
      "outputs": [],
      "source": [
        "## Tokenization\n",
        "## Paragraph-->words\n",
        "## sentence--->words\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Word Tokenization**"
      ],
      "metadata": {
        "id": "X7Fqc-nZFJGP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "iEKaIkV8_R3i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a3ff931-3abc-401b-b209-ce0d3b73cf88"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization:\n",
            "Sentence 1 words: ['Hello', 'Welcome', ',', 'to', 'Krish', 'Naik', \"'s\", 'NLP', 'Tutorials', '.']\n",
            "Sentence 2 words: ['Please', 'do', 'watch', 'the', 'entire', 'course', '!']\n",
            "Sentence 3 words: ['to', 'become', 'expert', 'in', 'NLP', '.']\n"
          ]
        }
      ],
      "source": [
        "# 2. Word Tokenizer (use TreebankWordTokenizer to bypass punkt_tab)\n",
        "word_tokenizer = TreebankWordTokenizer()\n",
        "\n",
        "print(\"Word Tokenization:\")\n",
        "for i, sentence in enumerate(sentences, 1):\n",
        "    words = word_tokenizer.tokenize(sentence)\n",
        "    print(f\"Sentence {i} words: {words}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "T3hnDCpj_R3k"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "WyLebNEn_R3l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03ee2738-50d1-49b9-803f-adf57ebcdcf8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'Welcome',\n",
              " ',',\n",
              " 'to',\n",
              " 'Krish',\n",
              " 'Naik',\n",
              " \"'\",\n",
              " 's',\n",
              " 'NLP',\n",
              " 'Tutorials',\n",
              " '.',\n",
              " 'Please',\n",
              " 'do',\n",
              " 'watch',\n",
              " 'the',\n",
              " 'entire',\n",
              " 'course',\n",
              " '!',\n",
              " 'to',\n",
              " 'become',\n",
              " 'expert',\n",
              " 'in',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "wordpunct_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import (word_tokenize,\n",
        "                          sent_tokenize,\n",
        "                          TreebankWordTokenizer,\n",
        "                          wordpunct_tokenize,\n",
        "                          TweetTokenizer,\n",
        "                          MWETokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ArCWnHdNWHY",
        "outputId": "0dc9b726-5045-497c-a0ca-ee683f1786df"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"Hope, is the only thing stronger that fear! #Hope #Amal.M\""
      ],
      "metadata": {
        "id": "bqcQVtXaNWd8"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Punctuation-based tokenizer\n",
        "print(wordpunct_tokenize(text))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5stzHi_NtP0",
        "outputId": "2d7ebb3c-d5ed-4d72-ddab-b8d332c242c6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hope', ',', 'is', 'the', 'only', 'thing', 'stronger', 'that', 'fear', '!', '#', 'Hope', '#', 'Amal', '.', 'M']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Treebank Word tokenizer\n",
        "text=\"What you don't want to be done to yourself, don't do to others...\"\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize(text))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dk1ZLn8N3Ca",
        "outputId": "b4a75f24-47b9-4d44-c40c-ef706b1c2c30"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['What', 'you', 'do', \"n't\", 'want', 'to', 'be', 'done', 'to', 'yourself', ',', 'do', \"n't\", 'do', 'to', 'others', '...']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tweet tokenizer\n",
        "tweet= \"Don't take cryptocurrency advice from people on Twitter ðŸ˜…ðŸ‘Œ\"\n",
        "tokenizer= TweetTokenizer()\n",
        "print(tokenizer.tokenize(tweet))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKIdS9VeN5a_",
        "outputId": "a4e0671a-9329-4208-d391-a1510b6926ea"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"Don't\", 'take', 'cryptocurrency', 'advice', 'from', 'people', 'on', 'Twitter', 'ðŸ˜…', 'ðŸ‘Œ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVQHjq3tQWzk",
        "outputId": "ab7e3069-505d-4428-a14e-80ac0e8d202a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "text = \"\"\"But I'm glad you'll see me as I am.\n",
        "Above all, I wouldn't want people to think that I want to prove anything.\n",
        "I don't want to prove anything, I just want to live;\n",
        "to cause no evil to anyone but myself.\n",
        "I have that right, haven't I? Leo Tolstoy\"\"\"\n",
        "\n",
        "blob_object = TextBlob(text)\n",
        "\n",
        "# Word tokenization\n",
        "text_words = blob_object.words\n",
        "\n",
        "print(text_words)\n",
        "print(len(text_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yry8EDoVOF7y",
        "outputId": "9a6d98ea-bb2d-4a0a-f3ef-811db32ff6c0"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['But', 'I', \"'m\", 'glad', 'you', \"'ll\", 'see', 'me', 'as', 'I', 'am', 'Above', 'all', 'I', 'would', \"n't\", 'want', 'people', 'to', 'think', 'that', 'I', 'want', 'to', 'prove', 'anything', 'I', 'do', \"n't\", 'want', 'to', 'prove', 'anything', 'I', 'just', 'want', 'to', 'live', 'to', 'cause', 'no', 'evil', 'to', 'anyone', 'but', 'myself', 'I', 'have', 'that', 'right', 'have', \"n't\", 'I', 'Leo', 'Tolstoy']\n",
            "55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "blob_object= TextBlob(text)\n",
        "#Word tokenization of  the text\n",
        "text_words = blob_object.words\n",
        "#To see all tokens\n",
        "print(text_words)\n",
        "#To count the number of tokens\n",
        "print(len(text_words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5U-MRpbOGO2",
        "outputId": "aedfa76e-46bc-4d39-a00d-f16cae3d2d9c"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['But', 'I', \"'m\", 'glad', 'you', \"'ll\", 'see', 'me', 'as', 'I', 'am', 'Above', 'all', 'I', 'would', \"n't\", 'want', 'people', 'to', 'think', 'that', 'I', 'want', 'to', 'prove', 'anything', 'I', 'do', \"n't\", 'want', 'to', 'prove', 'anything', 'I', 'just', 'want', 'to', 'live', 'to', 'cause', 'no', 'evil', 'to', 'anyone', 'but', 'myself', 'I', 'have', 'that', 'right', 'have', \"n't\", 'I', 'Leo', 'Tolstoy']\n",
            "55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **spaCy Tokenizer**"
      ],
      "metadata": {
        "id": "EWlibXCvRAnD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install spacy\n",
        "!python3 -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXbiSNmwQ_XF",
        "outputId": "8007f052-fbbe-4250-99b2-058c0fdc7327"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m94.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3mâš  Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from spacy.lang.en import English\n",
        "nlp = English()\n",
        "text= \"All happy families are alike; each unhappy family is unhappy in its own way!!! ðŸ‘ŒðŸ‘Œ #Leo Tolstoy\"\n",
        "doc = nlp(text)\n",
        "for token in doc:\n",
        "  print(token, token.idx)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfa0iIYXRIfu",
        "outputId": "f210feea-c177-47aa-f532-f4b00856bc06"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All 0\n",
            "happy 4\n",
            "families 10\n",
            "are 19\n",
            "alike 23\n",
            "; 28\n",
            "each 30\n",
            "unhappy 35\n",
            "family 43\n",
            "is 50\n",
            "unhappy 53\n",
            "in 61\n",
            "its 64\n",
            "own 68\n",
            "way 72\n",
            "! 75\n",
            "! 76\n",
            "! 77\n",
            "ðŸ‘Œ 79\n",
            "ðŸ‘Œ 80\n",
            "# 82\n",
            "Leo 83\n",
            "Tolstoy 87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Gensim Word tokenizer**"
      ],
      "metadata": {
        "id": "KO4KbAJGR4vq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClNyIseBR9x3",
        "outputId": "60d9e4f3-1f7a-4b56-9d4e-3522295ec397"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m65.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.utils import tokenize\n",
        "list(tokenize(text))\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-KnMKcaDR2rH",
        "outputId": "e2709c82-d758-4fef-d458-aa6f047b9098"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['All',\n",
              " 'happy',\n",
              " 'families',\n",
              " 'are',\n",
              " 'alike',\n",
              " 'each',\n",
              " 'unhappy',\n",
              " 'family',\n",
              " 'is',\n",
              " 'unhappy',\n",
              " 'in',\n",
              " 'its',\n",
              " 'own',\n",
              " 'way',\n",
              " 'Leo',\n",
              " 'Tolstoy']"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization with Keras**"
      ],
      "metadata": {
        "id": "beW0gQL8SUqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZLhpgEXSbYR",
        "outputId": "5c9d4f00-e418-425d-e21d-8bac58110259"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.12/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from keras) (2.0.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras) (0.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.12/dist-packages (from keras) (3.15.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras) (0.18.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.12/dist-packages (from keras) (0.5.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from keras) (25.0)\n",
            "Requirement already satisfied: typing-extensions>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from optree->keras) (4.15.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "text = \"\"\"But I'm glad you'll see me as I am.\n",
        "Above all, I wouldn't want people to think that I want to prove anything.\"\"\"\n",
        "\n",
        "# Create tokenizer\n",
        "ntoken = Tokenizer(num_words=20)\n",
        "\n",
        "# Fit tokenizer\n",
        "ntoken.fit_on_texts([text])   # MUST be a list\n",
        "\n",
        "# Convert text to word sequence\n",
        "list_words = text_to_word_sequence(text)\n",
        "\n",
        "print(list_words)\n",
        "print(len(list_words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lynArAlAREcE",
        "outputId": "c7b068a8-6ef8-4de6-aec4-4e3d2a3b8836"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['but', \"i'm\", 'glad', \"you'll\", 'see', 'me', 'as', 'i', 'am', 'above', 'all', 'i', \"wouldn't\", 'want', 'people', 'to', 'think', 'that', 'i', 'want', 'to', 'prove', 'anything']\n",
            "23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J7s8B5irSZDl"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}